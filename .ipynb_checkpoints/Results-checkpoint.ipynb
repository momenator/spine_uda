{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results visualisation\n",
    "\n",
    "We visualise results of trained networks on the mr_sag_kr/test* datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Declare net here\n",
    "\n",
    "for example:\n",
    "\n",
    "net = UNetWCT('./saved_models/ct_unetwct/encoder ct_unetwct',\n",
    "              './saved_models/ct_unetwct/decoder ct_unetwct',\n",
    "              # 10, 1, [25, 54, 27, 22, 52, 16, 8 ,4, 36, 7])\n",
    "              6, 1, [4,7,11,24,25,27])\n",
    "\n",
    "net.UNet.load_state_dict(\n",
    "            torch.load(os.path.join('./saved_models/ct_unetwct/ct_unetwct'),\n",
    "                       map_location=lambda storage, loc: storage))\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from models.unet import UNet, UNetDense, UNetWavelet\n",
    "\n",
    "# net = UNet(1, 1, norm_layer=nn.BatchNorm2d, affine=True)\n",
    "# net = UNet(1, 1, norm_layer=nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "# net = UNet(1, 1)\n",
    "# net = UNetDense(1, 1, norm_layer=nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "# net = UNetWavelet(1, 1)\n",
    "\n",
    "net = UNet(1, 1)\n",
    "# net = UNet(1, 1, domain_specific=True)\n",
    "# refine_net = UNet(1, 1, norm_layer=nn.BatchNorm2d, affine=True, track_running_stats=True)\n",
    "\n",
    "# outputs to test?\n",
    "\n",
    "# plain_unet_sobel - baseline # 1 0.62 dice\n",
    "# exp10_unet_sobel_2 - baseline # 2 0.777\n",
    "\n",
    "# 10 batch size\n",
    "# best one - mcd_sobel_exp2 # 3 0.832\n",
    "# alternative - unet_sobel_swd_eadan_in_10_batch_20_epochs, mcd_induced_exp4\n",
    "\n",
    "net.cuda()\n",
    "net.load_state_dict(torch.load(os.path.join('./results/unet_sobel_swd_eadan_in_10_batch_20_epochs/net'), \n",
    "                               map_location=lambda storage, loc: storage))\n",
    "\n",
    "# net.load_state_dict(torch.load(os.path.join('./results/dsbn_final/ds_unet'), map_location=lambda storage, loc: storage))\n",
    "# net.set_domain(1)\n",
    "\n",
    "\n",
    "# refine_net.cuda()\n",
    "# refine_net.load_state_dict(torch.load(os.path.join('./results/unet_shape_model_2/refiner'), \n",
    "#                                map_location=lambda storage, loc: storage))\n",
    "\n",
    "# net.set_domain(1)\n",
    "\n",
    "# summary(net, (1, 256, 256))\n",
    "\n",
    "# pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "# print(pytorch_total_params)\n",
    "\n",
    "# for n, p in net.named_parameters():\n",
    "#     print(n, p.numel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # WCT2 stuff\n",
    "# from models.wct2 import WCT2Features\n",
    "\n",
    "\n",
    "# wct2net = WCT2Features([25,54,16,22,61,4,8,27,7,3], \n",
    "#                        './wct2_weights/wave_encoder_cat5_l4.pth', \n",
    "#                        './wct2_weights/wave_decoder_cat5_l4.pth').cuda()\n",
    "\n",
    "# wct2net.load_state_dict(torch.load(os.path.join('./results/exp6_wct2_gan_unet_instancenorm/g.pth'), \n",
    "#                                    map_location=lambda storage, loc: storage))\n",
    "\n",
    "\n",
    "# net = UNet(10, 1)\n",
    "# net.cuda()\n",
    "# net.load_state_dict(torch.load(os.path.join('./results/exp6_wct2_gan_unet_instancenorm/seg.pth'),\n",
    "#                                map_location=lambda storage, loc: storage))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put dataset here!\n",
    "from torch.utils.data import DataLoader\n",
    "from data.dataset import UnpairedDataset\n",
    "# 21, 22, 15, 23, 16\n",
    "seed = 21\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "ct_scans = UnpairedDataset('../', path_a='ct_sag_kr/test', path_b=None, corrupt=True)\n",
    "all_scans = UnpairedDataset('../', path_a='mr_sag_kr/test', path_b=None)\n",
    "gold_scans = UnpairedDataset('../', path_a='mr_sag_kr/test_gold', path_b=None)\n",
    "std_scans = UnpairedDataset('../', path_a='mr_sag_kr/test_std', path_b=None)\n",
    "paired = UnpairedDataset('../', path_a='ct_sag_kr/test', path_b='mr_sag_kr/test_gold')\n",
    "\n",
    "ct_loader = DataLoader(ct_scans, batch_size=1, shuffle=False, num_workers=5, pin_memory=True)\n",
    "all_loader = DataLoader(all_scans, batch_size=1, shuffle=True, num_workers=5, pin_memory=True)\n",
    "gold_loader = DataLoader(gold_scans, batch_size=1, shuffle=False, num_workers=5, pin_memory=True)\n",
    "std_loader = DataLoader(std_scans, batch_size=1, shuffle=True, num_workers=5, pin_memory=True)\n",
    "paired_loader = DataLoader(paired, batch_size=1, shuffle=False, num_workers=5, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.21964511275291443\n",
      "1 1 0.6962137222290039\n",
      "2 2 0.7534416317939758\n",
      "3 3 0.6217048764228821\n",
      "4 4 0.7586506009101868\n",
      "5 5 0.6934896111488342\n",
      "6 6 0.6663137674331665\n",
      "7 7 0.6939050555229187\n",
      "8 8 0.7837637662887573\n",
      "9 9 0.7838422656059265\n",
      "10 10 0.6853162050247192\n",
      "11 11 0.820452868938446\n",
      "12 12 0.6633944511413574\n",
      "13 13 0.6997960805892944\n",
      "14 14 0.6520469784736633\n",
      "15 15 0.7479085326194763\n",
      "16 16 0.538879930973053\n",
      "17 17 0.7984675168991089\n",
      "18 18 0.7856885194778442\n",
      "19 19 0.6472480297088623\n",
      "20 20 0.6848403215408325\n",
      "21 21 0.6450944542884827\n",
      "22 22 0.12597912549972534\n",
      "23 23 0.5940373539924622\n",
      "24 24 0.6794880032539368\n",
      "25 25 0.667599618434906\n",
      "26 26 0.5314525961875916\n",
      "27 27 0.3879271149635315\n",
      "28 28 0.6093923449516296\n",
      "29 29 0.5748364925384521\n",
      "30 30 0.5003675222396851\n",
      "31 31 0.41392946243286133\n",
      "32 32 0.6468284130096436\n",
      "33 33 0.8313360214233398\n",
      "34 34 0.5225132703781128\n",
      "35 35 0.3136290907859802\n",
      "36 36 0.7046892046928406\n",
      "37 37 0.6692537069320679\n",
      "38 38 0.6479681134223938\n",
      "39 39 0.7354329824447632\n",
      "40 40 0.21376724541187286\n",
      "41 41 0.7640408873558044\n",
      "42 42 0.08093172311782837\n",
      "43 43 0.6864258646965027\n",
      "44 44 0.7671844959259033\n",
      "45 45 0.15173766016960144\n",
      "46 46 0.6693838834762573\n",
      "47 47 0.752255916595459\n",
      "48 48 0.7462133765220642\n",
      "49 49 0.6020052433013916\n",
      "50 50 0.6644149422645569\n",
      "51 51 0.6358566880226135\n",
      "52 52 0.8259358406066895\n",
      "53 53 0.7020547986030579\n",
      "54 54 0.781746506690979\n",
      "55 55 0.7569756507873535\n",
      "56 56 0.7525402307510376\n",
      "57 57 0.7418598532676697\n",
      "58 58 0.6755878925323486\n",
      "59 59 0.7641574740409851\n",
      "60 60 0.3905882239341736\n",
      "61 61 0.7820813655853271\n",
      "62 62 0.8027145862579346\n",
      "63 63 0.800069272518158\n",
      "64 64 0.7496796250343323\n",
      "65 65 0.7521844506263733\n",
      "66 66 0.6680715084075928\n",
      "67 67 0.7306568622589111\n",
      "68 68 0.7658603191375732\n",
      "69 69 0.8559589385986328\n",
      "70 70 0.5551850199699402\n",
      "71 71 0.7990790605545044\n",
      "72 72 0.39246559143066406\n",
      "73 73 0.424095094203949\n",
      "74 74 0.7913022637367249\n",
      "75 75 0.5001667141914368\n",
      "76 76 0.7410865426063538\n",
      "77 77 0.7783296704292297\n",
      "78 78 0.6496759653091431\n",
      "79 79 0.661129891872406\n",
      "80 80 0.16223713755607605\n",
      "81 81 0.5544584393501282\n",
      "82 82 0.7819679379463196\n",
      "83 83 0.32675182819366455\n",
      "84 84 0.48541754484176636\n",
      "85 85 0.22473101317882538\n",
      "86 86 0.7003750801086426\n",
      "87 87 0.7876777052879333\n",
      "88 88 0.7613471746444702\n",
      "90 89 0.8183286190032959\n",
      "91 90 0.37235724925994873\n",
      "92 91 0.5365661382675171\n",
      "93 92 0.6106261610984802\n",
      "94 93 0.4429725110530853\n",
      "95 94 0.3379398584365845\n",
      "96 95 0.6646971702575684\n",
      "97 96 0.6379332542419434\n",
      "98 97 0.6629376411437988\n",
      "99 98 0.6473606824874878\n",
      "100 99 0.1184939369559288\n",
      "101 100 0.7136228680610657\n",
      "102 101 0.6688784956932068\n",
      "103 102 0.6436934471130371\n",
      "104 103 0.3208475112915039\n",
      "106 104 0.442195326089859\n",
      "107 105 0.7688891291618347\n",
      "108 106 0.11943639814853668\n",
      "109 107 0.6601682901382446\n",
      "111 108 0.8164885640144348\n",
      "112 109 0.09457095712423325\n",
      "113 110 0.688684344291687\n",
      "114 111 0.5285432934761047\n",
      "115 112 0.7820748090744019\n",
      "116 113 0.22125890851020813\n",
      "117 114 0.38615575432777405\n",
      "118 115 0.39098885655403137\n",
      "119 116 0.659056544303894\n",
      "120 117 0.3810913562774658\n",
      "121 118 0.6625067591667175\n",
      "122 119 0.7449169158935547\n",
      "123 120 0.7299076318740845\n",
      "124 121 0.6896442770957947\n",
      "125 122 0.1873629093170166\n",
      "126 123 0.7323575615882874\n",
      "127 124 0.3928890824317932\n",
      "128 125 0.679673969745636\n",
      "129 126 0.5774512887001038\n",
      "130 127 0.7200447916984558\n",
      "131 128 0.573313295841217\n",
      "132 129 0.628796398639679\n",
      "133 130 0.7083166837692261\n",
      "134 131 0.7035567760467529\n",
      "135 132 0.7050062417984009\n",
      "136 133 0.6530224084854126\n",
      "137 134 0.7477245926856995\n",
      "138 135 0.8116118907928467\n",
      "139 136 0.7150630950927734\n",
      "140 137 0.8063241243362427\n",
      "141 138 0.7842603921890259\n",
      "142 139 0.767944872379303\n",
      "143 140 0.7316896915435791\n",
      "144 141 0.3533827066421509\n",
      "145 142 0.7375398874282837\n",
      "146 143 0.8046377301216125\n",
      "147 144 0.8018128275871277\n",
      "148 145 0.38997527956962585\n",
      "149 146 0.7567731142044067\n",
      "150 147 0.6260292530059814\n",
      "151 148 0.7430927157402039\n",
      "152 149 0.7462556958198547\n",
      "153 150 0.5030123591423035\n",
      "154 151 0.697600245475769\n",
      "155 152 0.7639430165290833\n",
      "156 153 0.6712289452552795\n",
      "157 154 0.7405268549919128\n",
      "158 155 0.5808695554733276\n",
      "159 156 0.761422872543335\n",
      "160 157 0.08102607727050781\n",
      "161 158 0.3672179579734802\n",
      "162 159 0.6495242118835449\n",
      "163 160 0.4651462733745575\n",
      "164 161 0.6533903479576111\n",
      "165 162 0.7861915230751038\n",
      "166 163 0.7171828746795654\n",
      "167 164 0.2795206606388092\n",
      "168 165 0.7828382253646851\n",
      "169 166 0.4040263891220093\n",
      "170 167 0.20328469574451447\n",
      "172 168 0.6874939203262329\n",
      "173 169 0.7449901103973389\n",
      "174 170 0.5673015713691711\n",
      "175 171 0.7139104604721069\n",
      "176 172 0.7259186506271362\n",
      "177 173 0.7054553627967834\n",
      "178 174 0.5545624494552612\n",
      "179 175 0.6484119296073914\n",
      "180 176 0.4465886652469635\n",
      "181 177 0.561223566532135\n",
      "183 178 0.6949286460876465\n",
      "184 179 0.7846104502677917\n",
      "185 180 0.8621416687965393\n",
      "186 181 0.45001617074012756\n",
      "187 182 0.8329849243164062\n",
      "188 183 0.6932926177978516\n",
      "189 184 0.6115854382514954\n",
      "190 185 0.21889057755470276\n",
      "191 186 0.6014058589935303\n",
      "192 187 0.6348242163658142\n",
      "193 188 0.28089290857315063\n",
      "194 189 0.834606945514679\n",
      "195 190 0.6146327257156372\n",
      "196 191 0.4830681383609772\n",
      "197 192 0.5566053986549377\n",
      "198 193 0.7347701787948608\n",
      "199 194 0.6978672742843628\n",
      "200 195 0.7521407008171082\n",
      "201 196 0.7402705550193787\n",
      "202 197 0.7890990972518921\n",
      "203 198 0.6607002019882202\n",
      "204 199 0.09423817694187164\n",
      "205 200 0.6685883402824402\n",
      "206 201 0.6677496433258057\n",
      "207 202 0.24068160355091095\n",
      "208 203 0.7417840361595154\n",
      "209 204 0.4488658010959625\n",
      "210 205 0.7977893352508545\n",
      "211 206 0.4503496587276459\n",
      "212 207 0.7007614970207214\n",
      "213 208 0.6676262617111206\n",
      "214 209 0.762103259563446\n",
      "215 210 0.7600947618484497\n",
      "216 211 0.7462841868400574\n",
      "217 212 0.6571007370948792\n",
      "218 213 0.6393119096755981\n",
      "219 214 0.7839543223381042\n",
      "220 215 0.6630876660346985\n",
      "221 216 0.7469879388809204\n",
      "222 217 0.5508173704147339\n",
      "223 218 0.6919382214546204\n",
      "224 219 0.7253216505050659\n",
      "226 220 0.6974527835845947\n",
      "227 221 0.5656192302703857\n",
      "228 222 0.6671214699745178\n",
      "229 223 0.7112412452697754\n",
      "230 224 0.7523235082626343\n",
      "231 225 0.7758811116218567\n",
      "232 226 0.7799218893051147\n",
      "233 227 0.859040379524231\n",
      "234 228 0.7998500466346741\n",
      "235 229 0.6947616338729858\n",
      "236 230 0.7393025755882263\n",
      "237 231 0.6942551732063293\n",
      "238 232 0.7731725573539734\n",
      "239 233 0.7250537872314453\n",
      "240 234 0.7751018404960632\n",
      "241 235 0.5392819046974182\n",
      "242 236 0.7895389199256897\n",
      "243 237 0.5193301439285278\n",
      "245 238 0.7502204775810242\n",
      "246 239 0.6971954107284546\n",
      "247 240 0.662552535533905\n",
      "248 241 0.7600115537643433\n",
      "249 242 0.33900150656700134\n",
      "250 243 0.6544301509857178\n",
      "251 244 0.788939893245697\n",
      "252 245 0.700659990310669\n",
      "253 246 0.31669139862060547\n",
      "254 247 0.7852143049240112\n",
      "255 248 0.8513954281806946\n",
      "257 249 0.7780976295471191\n",
      "258 250 0.11113408952951431\n",
      "259 251 0.8534790873527527\n",
      "260 252 0.47025802731513977\n",
      "261 253 0.4214220643043518\n",
      "262 254 0.4262966215610504\n",
      "263 255 0.6392799615859985\n",
      "264 256 0.711929976940155\n",
      "265 257 0.31084442138671875\n",
      "266 258 0.7200000286102295\n",
      "267 259 0.8595781922340393\n",
      "268 260 0.21575923264026642\n",
      "269 261 0.4297274351119995\n",
      "270 262 0.3092228174209595\n",
      "271 263 0.17573966085910797\n",
      "272 264 0.7939500212669373\n",
      "273 265 0.5920138955116272\n",
      "274 266 0.6511435508728027\n",
      "275 267 0.5849549174308777\n",
      "276 268 0.7929086089134216\n",
      "277 269 0.7370145320892334\n",
      "279 270 0.5769798159599304\n",
      "280 271 0.7772238254547119\n",
      "281 272 0.5853537321090698\n",
      "282 273 0.8040428757667542\n",
      "283 274 0.7241265177726746\n",
      "284 275 0.5479180812835693\n",
      "285 276 0.7022954821586609\n",
      "286 277 0.6838101744651794\n",
      "287 278 0.6901408433914185\n",
      "288 279 0.6911091804504395\n",
      "289 280 0.2500542104244232\n",
      "290 281 0.7070906758308411\n",
      "291 282 0.6626125574111938\n",
      "292 283 0.22920356690883636\n",
      "293 284 0.7615252733230591\n",
      "294 285 0.22626909613609314\n",
      "295 286 0.8511796593666077\n",
      "296 287 0.31448277831077576\n",
      "mean dice  0.6201050190720707\n",
      "mean h95  8.964164480860541\n",
      "mean recall  0.14596170519578952\n"
     ]
    }
   ],
   "source": [
    "# Visualise results here!\n",
    "import matplotlib.pyplot as plt\n",
    "from utils.metrics import compute_dice_metric\n",
    "from PIL import Image\n",
    "from utils.fda import FDA_source_to_target\n",
    "\n",
    "# metrics\n",
    "# recall and F1\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "\n",
    "\n",
    "def threshold_prediction(img, threshold=0.8):\n",
    "    return (img > threshold).float()\n",
    "\n",
    "\n",
    "def show_image(img):\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_histo(arr):\n",
    "    non_zeros = np.array([el for el in arr.ravel() if el != 0])\n",
    "#     print(len(non_zeros))\n",
    "    plt.hist(non_zeros)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "loader = gold_loader\n",
    "\n",
    "dices = []\n",
    "h95s = []\n",
    "recalls = []\n",
    "\n",
    "# i = 8 or 32 for visuals - 8 for mr and 32 for CT\n",
    "# i = 54 for CT\n",
    "count = 0\n",
    "\n",
    "for i, data in enumerate(loader):\n",
    "    # test 250 images\n",
    "        \n",
    "    image = data['A'][0].cuda()\n",
    "    mask = data['A'][1].cuda()\n",
    "    edge = data['A'][2].cuda()\n",
    "    # corr = data['A'][3].cuda()\n",
    "    mask_np = mask.squeeze().cpu().numpy()\n",
    "    ratio = (mask_np.sum() / np.prod(mask_np.shape))\n",
    "    \n",
    "    # for wct2\n",
    "#     with torch.no_grad():\n",
    "#         image = torch.cat([image,image,image], dim=1)\n",
    "#         image = wct2net(image)\n",
    "\n",
    "#     if ratio < 0.01:\n",
    "#         continue\n",
    "    \n",
    "#     image_target = data['A'][0].cuda()\n",
    "#     image = FDA_source_to_target(image, image_target, 0.02).cuda()\n",
    "    \n",
    "    if torch.max(mask) == 0:\n",
    "        continue\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = net(edge)\n",
    "        \n",
    "#         pred_nums = pred.squeeze().cpu().numpy()\n",
    "#         plot_histo(pred_nums)\n",
    "\n",
    "#         print((pred > 0.3).shape)\n",
    "#         pred = torch.round(net(edge))\n",
    "        factor = 0.5\n",
    "        pred = threshold_prediction(pred, 0.5)\n",
    "#         pred_ref = refine_net(pred)\n",
    "#         pred_ref = threshold_prediction(pred_ref, 0.5)\n",
    "#         pred = threshold_prediction(factor * pred + (1 - factor) * pred_ref, 0.9)\n",
    "        dice = compute_dice_metric(pred, mask).item()\n",
    "        \n",
    "    image_np = image.squeeze().cpu().numpy()\n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    print(i, count, dice)\n",
    "#     show_image(image_np)\n",
    "#     show_image(mask_np)\n",
    "#     show_image(pred_np)\n",
    "#     show_image(corr.squeeze().cpu().numpy())\n",
    "    dices.append(dice)\n",
    "\n",
    "    # compute robust hausdorff (0.95)\n",
    "    h95 = 0.95 * max(directed_hausdorff(mask_np, pred_np)[0], directed_hausdorff(pred_np, mask_np)[0])\n",
    "\n",
    "    # print(f1, recall, h95)\n",
    "    h95s.append(h95)\n",
    "    \n",
    "    rec = recall_score(mask_np, pred_np, labels=[0, 1], average='micro')\n",
    "    recalls.append(rec)\n",
    "    \n",
    "    # plt.imsave('./isbi_results/ct_adapt.png', image_adapt_np, cmap='gray')\n",
    "    # plt.imsave('./isbi_results/ct_edge.png', image_edge, cmap='gray')\n",
    "    # plt.imsave('./isbi_results/ct_seg.png', pred_np, cmap='gray')\n",
    "\n",
    "    # save the image here!\n",
    "    count += 1\n",
    "    # break\n",
    "\n",
    "print('mean dice ', np.mean(dices))\n",
    "print('mean h95 ', np.mean(h95s))\n",
    "print('mean recall ', np.mean(recalls))\n",
    "\n",
    "# 0.160, 0.16, 0.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 vals for swd eadanin\n",
    "\n",
    "# wct handpicked\n",
    "mean dice  0.8464936307103998\n",
    "mean h95  3.5863614651415543\n",
    "mean recall  0.4260145276266741\n",
    "\n",
    "mean dice  0.7905552176051882\n",
    "mean h95  4.035267249723969\n",
    "mean recall  0.40235083700715685\n",
    "\n",
    "mean dice  0.6544912152716683\n",
    "mean h95  8.827009998470999\n",
    "mean recall  0.14791761912555945"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 746 CT slice, 287 MR slice\n",
    "\n",
    "# Thesis results\n",
    "# 01 - CT - 0.759, MR - 0.156\n",
    "# 02 - CT - 0.767, MR - 0.150\n",
    "# 03 - CT - 0.769, MR - 0.128\n",
    "# 04 - CT - 0.650, MR - 0.230\n",
    "# 05 - CT - 0.733, MR - 0.163\n",
    "\n",
    "# 06 - CT - 0.747, MR - 0.151\n",
    "# 07 - CT - 0.724, MR - 0.109\n",
    "# 08 - CT - 0.838, MR - 0.214\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ISBI results\n",
    "# put results here dice, hausdorff and respective std! - put results in array!\n",
    "\n",
    "# plain_unet_final - plain UNet\n",
    "# dices = [0.10037092795650356, 0.10177221070904102, 0.09924121791935495, 0.10009683817874111, 0.09914480147642214]\n",
    "# hausdorf = [10.145244400423616, 10.140494663252197, 10.16344320388977, 10.131213088551885, 10.195365737283321]\n",
    "\n",
    "# plain_mcd_final - plain mcd\n",
    "# dices = [0.137577205368192, 0.1383025814779305, 0.13659828514812897, 0.13542503835725136, 0.1377713427699978]\n",
    "# hausdorf = [10.366328330703178, 10.414881056077668, 10.410492704361262, 10.424023283755947, 10.416536909222229]\n",
    "\n",
    "# plain_sliced_mcd_final - mcd with SWD\n",
    "# dices = [0.14690331194415326, 0.14723133917977818, 0.14691833064815335, 0.14658343207858043, 0.14594228413768492]\n",
    "# hausdorf = [10.171175449513028, 10.150920429196855, 10.169769583407106, 10.162861798201325, 10.188987370737019]\n",
    "\n",
    "# final_fda_03 - FDA with 0.02 amplitude scaling - do this one later!\n",
    "# dices = [0.1264268740741407, 0.1264395040361679, 0.12500124313956734, 0.12558176408006913, 0.12395977855262859]\n",
    "# hausdorf = [10.319820976996066, 10.305481187686135, 10.347600221595291, 10.364132762083221, 10.37480992571693]\n",
    "\n",
    "# plain_unet_sobel - baseline # 1 0.62 dice\n",
    "# dices = [0.4808360231350143, 0.48269747731932605, 0.4767395307521421, 0.4770458111042283, 0.4756741902802095]\n",
    "# hausdorf = [9.4269376811709, 9.453275622137463, 9.459829273045996, 9.474492785367403, 9.51973747827922]\n",
    "\n",
    "# unet_sobel + adabn\n",
    "# dices = [0.5489367615297971, 0.5533777223343394, 0.5514127915183387, 0.5461562726780238, 0.547703714811232]\n",
    "# hausdorf = [9.120269902294087, 9.122406943913955, 9.147277630846691, 9.164941209212648, 9.165092111395293]\n",
    "\n",
    "\n",
    "# exp10_unet_sobel_2 - baseline + normalization # 2 0.777\n",
    "# dices = [0.55369083314904, 0.558332289645144, 0.554021822318316, 0.5518667476837972, 0.5543983771461505]\n",
    "# hausdorf = [9.071535876726662,  9.031207651446062, 9.085507623194724, 9.079528838247237, 9.091025266777653]\n",
    "\n",
    "# SWD + sobel\n",
    "# dices = [0.5798531098076072, 0.5763231502081531, 0.5772444732873089, 0.5782180370770366, 0.5792168695108587]\n",
    "# hausdorf = [9.386991331475674, 9.387046500364585, 9.422780042628858, 9.404803264137765, 9.435728359258675]\n",
    "\n",
    "# mcd_sobel_exp2 - SPINAL (best network) # 3 0.832\n",
    "# dices = [0.6468020982416978, 0.646971153785508, 0.6462393136197827, 0.6457881888307898, 0.6476100527373443]\n",
    "# hausdorf = [8.909019706080347, 8.90599351357345, 8.937906509219324, 8.935234939831625, 8.941844710098946]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean dice  0.5495174525743461\n",
      "std dice  0.002585280232026687\n",
      "mean hdf  9.143997559532533\n",
      "std hdf  0.019613933052064627\n"
     ]
    }
   ],
   "source": [
    "# dont put std as most std are lower than the given precisions\n",
    "dices = [0.5489367615297971, 0.5533777223343394, 0.5514127915183387, 0.5461562726780238, 0.547703714811232]\n",
    "hausdorf = [9.120269902294087, 9.122406943913955, 9.147277630846691, 9.164941209212648, 9.165092111395293]\n",
    "\n",
    "print('mean dice ', np.mean(dices))\n",
    "print('std dice ', np.std(dices))\n",
    "\n",
    "print('mean hdf ', np.mean(hausdorf))\n",
    "print('std hdf ', np.std(hausdorf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "(3,) (3,)\n",
      "[1 0 0]\n",
      "0.6666666666666666 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# 0.23376623376623376 0.13636363636363635 8.911789943664516\n",
    "\n",
    "def compute_f1(p, g):\n",
    "    p = p.ravel()\n",
    "    g = g.ravel()    \n",
    "    pass\n",
    "\n",
    "# compute recall on foreground\n",
    "def compute_recall(p, g):\n",
    "    p = p.ravel()\n",
    "    g = g.ravel()\n",
    "    print(p.shape, g.shape)\n",
    "    fn = np.logical_and(p==0,g==1).astype(np.uint8)\n",
    "    print(fn)\n",
    "    return p.sum() / (p.sum() + fn.sum())\n",
    "\n",
    "\n",
    "# recall - 3 / 3 + 1\n",
    "# TP - 3\n",
    "# FN - 1\n",
    "\n",
    "p = np.array([0, 1, 1])\n",
    "g = np.array([1, 1, 1])\n",
    "\n",
    "print(np.logical_and(p==0,g==1).astype(np.uint8))\n",
    "\n",
    "res1 = compute_recall(p, g)\n",
    "res2 = recall_score(p, g, labels=[0, 1], average='micro')\n",
    "print(res1, res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.Tensor([1,2])>1.5).float() * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
